{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0de7be73",
   "metadata": {},
   "source": [
    "#  DATA 364 Final Project Part 3 - MarketEdge Pro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97461a0e",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0469dcd7",
   "metadata": {},
   "source": [
    "Our project aims to develop an automated stock trading algorithm that leverages key technical indicators, specifically Relative Strength Index (RSI), momentum, and moving average crossovers. This algorithm will provide traders with an efficient tool to make informed buy and sell decisions in financial markets. By combining these indicators, our system seeks to capture valuable trading opportunities and enhance portfolio performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d80f7ec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77f9daea",
   "metadata": {},
   "source": [
    "## Data Extraction Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89cd844",
   "metadata": {},
   "source": [
    "We initially looked at several different methods of retreiving our data, including web scraping, pulling from premade datasets, and using various APIs. We quickly ruled out using premade datasets because we wanted to pull current stock information so our tool would be useful in analyzing stocks in the current market conditions. We also tried using web scraping originally on Yahoo Finance and Google Finance but ran into issues with both platforms. Yahoo Finance used to be a great solution for scraping data since it was formatted in HTML that was fast and easy to parse using libraries like Beautiful Soup. However, since last year, they changed their platform to make it very challenging to scrape information, requireing the use of an application like Selenium. \n",
    "\n",
    "Similarly, Google Finance is very sensitive to web scraping and only allows requests every 60 seconds, limiting the amount of stocks you can analyze and making the overall user experience unsatisfactory. We eneded up using Yahoo Finance's API, which provides quick and up-to-date data in a dataframe that is ready for analysis. This method allows us to retrieve the most current data so the user will always get the most current information for their analysis. \n",
    "\n",
    "We are also using the Alpha Vantage API to retrieve real-time stock information when the market is open for additional information throughout the day. Finally, we are using the News API (NewsAPI.org) that will search for the top news articles for the given company (based on the stock ticker symbol) and display the top five articles for the user to read and gain better awareness of how the company is performing any pertinent inforamtion that may be affecting the stock price. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cf0545",
   "metadata": {},
   "source": [
    "## Extract Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started with the project, ensure you install the correct packages by running the pip command below. We are using Streamlit for our web interface, so if you would like to run the code, please use the .py file included in the folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dcb7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up instructions: Install the required packages using the following command:\n",
    "## pip install streamlit yfinance prophet requests plotly pandas ta\n",
    "## Run the app using the following command:\n",
    "## streamlit run main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to import the required packages to run this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd277ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import streamlit as st\n",
    "from datetime import date, time, datetime\n",
    "import yfinance as yf\n",
    "from prophet import Prophet\n",
    "import requests\n",
    "from prophet.plot import plot_plotly\n",
    "from plotly import graph_objs as go\n",
    "import pandas as pd\n",
    "import ta\n",
    "import os, contextlib\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d18b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set start and end dates. The start date can be configured to be any date in the past, arbitrarily set to 2015-01-01. \n",
    "START = \"2015-01-01\"\n",
    "TODAY = date.today().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2125e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_dir = \"stocks\"\n",
    "dataframes = []\n",
    "data = None  # Initialize data in the global scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Alpha Vantage and NewsAPI APIs to get our realtime stock data as well as our news data. You can get your own API key from the following links: \n",
    "\n",
    "NewsAPI: https://newsapi.org/register\n",
    "\n",
    "Alpha Vantage: https://www.alphavantage.co/support/#api-key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f047307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alpha Vantage API key\n",
    "realtime_api_key = 'IDKN0O488FIDEPVR'\n",
    "\n",
    "#News API (https://newsapi.org/) Key\n",
    "news_api_key = '60d16a8cc922453896de9495f9b99b1d'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can set the various treshholds for our RSI and MACD indicators. This allows us to easily modify our parameters for our buy/sell indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b559de72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set threshold values for the indicators\n",
    "rsi_buy_threshold = 45  # Lower RSI values suggest buying opportunities\n",
    "rsi_sell_threshold = 55  # Higher RSI values suggest selling opportunities\n",
    "macd_buy_threshold = 0  # Higher MACD values relative to the signal line suggest buying opportunities\n",
    "macd_sell_threshold = 0  # Lower MACD values relative to the signal line suggest selling opportunities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5e4e0b",
   "metadata": {},
   "source": [
    "### API 1 - Historical Stock Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2084a2fd",
   "metadata": {},
   "source": [
    "This function will retrieve and load the historical stock information using the Yahoo Finance API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aedbfc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def download_stock_data():\n",
    "    offset = 0\n",
    "    limit = 3000\n",
    "    period = '5y'  # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
    "\n",
    "    if not os.path.exists(stocks_dir):\n",
    "        os.makedirs(stocks_dir)\n",
    "        print(f\"The folder '{stocks_dir}' has been created.\")\n",
    "    else:\n",
    "        print(f\"The folder '{stocks_dir}' already exists.\")\n",
    "\n",
    "    symbols = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]['Symbol'].tolist()\n",
    "    limit = limit if limit else len(symbols)\n",
    "    end = min(offset + limit, len(symbols))\n",
    "    is_valid = [False] * len(symbols)\n",
    "    st.info(\"Downloading stock data for the first time. This may take a few minutes.\")\n",
    "\n",
    "    # force silencing of verbose API\n",
    "    with open(os.devnull, 'w') as devnull:\n",
    "        with contextlib.redirect_stdout(devnull):\n",
    "            for i in range(offset, end):\n",
    "                s = symbols[i]\n",
    "                data = yf.download(s, period=period)\n",
    "                if len(data.index) == 0:\n",
    "                    continue\n",
    "\n",
    "                is_valid[i] = True\n",
    "                data.to_csv(join(stocks_dir, '{}.csv'.format(s)))\n",
    "\n",
    "    print('Total number of valid symbols downloaded = {}'.format(sum(is_valid)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db634f2",
   "metadata": {},
   "source": [
    "### API 2 - Current News Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b678a63e",
   "metadata": {},
   "source": [
    "This function will search for the top news stories for a company using the News API (from the ticket symbol provided by the user) and display the top five stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f518ba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to fetch top news stories\n",
    "def get_top_news(selected_stock):\n",
    "    news_url = f'https://newsapi.org/v2/everything?q={selected_stock}&apiKey={news_api_key}'\n",
    "\n",
    "    try:\n",
    "        response = requests.get(news_url)\n",
    "        news_data = response.json()\n",
    "\n",
    "        if news_data.get('status') == 'ok' and news_data.get('totalResults') > 0:\n",
    "            articles = news_data['articles']\n",
    "\n",
    "            # Display the top news stories\n",
    "            for article in articles[:5]:  # Display the top 5 news stories\n",
    "                st.markdown(f\"**Title:** {article['title']}\")\n",
    "                st.write(f\"**Description:** {article['description']}\")\n",
    "                st.markdown(f\"**Source:** {article['source']['name']}\")\n",
    "                st.write(f\"**Published At:** {article['publishedAt']}\")\n",
    "                st.write(f\"**URL:** {article['url']}\")\n",
    "                st.write(\"---\")\n",
    "        else:\n",
    "            st.warning(\"The market is currently close. Try again later\")\n",
    "\n",
    "    except Exception as e:\n",
    "        st.error(f\"An error occurred while fetching news: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fa25c8",
   "metadata": {},
   "source": [
    "### API 3 - Real Time Stock Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b7ede5",
   "metadata": {},
   "source": [
    "This function will retrieve the current stock information if the market is open using the Alpha Vantage API. If the market is closed, it will display a warning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17d81ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get real-time stock data\n",
    "def realtime_stock_data(api_key, symbol):\n",
    "    try:\n",
    "        realtime_stock_url = f'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol={symbol}&interval=5min&apikey={api_key}'\n",
    "        response = requests.get(realtime_stock_url)\n",
    "        stock_data = response.json()\n",
    "        return stock_data.get('Time Series (5min)', {})\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a17f81",
   "metadata": {},
   "source": [
    "## Data Transformation Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provided the option for offline usage of the application for user flexbility. If the user already has the stocks data, the program processes the existing data files within the directory to construct the combined dataset. For each stock, the data is read in, cleaned, and returned. If not, the program initiates the download of stock data using the download_stock_data() function. The ticker name is added, the date is re-formatted to a date for streamlined date functions down the line, and the new data is added to the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646d6dd7",
   "metadata": {},
   "source": [
    "### Loading and Merging Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we can begin consolidating the raw data and prepare it for the load phase. In order to do this, we will take all the .csv datasets and combine them into one large Pandas dataframe. Afterwards, we'll begin cleaning the data and find/fix any irregularities to ensure the data is pristine and ready for the final phase of the ETL process. \n",
    "\n",
    " This code iterates through each file in the stocks directory, reads the data into a dataframe, adds a new column for the ticker symbol (to use as in index), converts the date column to the correct format, and then appends it to the list. \n",
    "\n",
    " After the loop, we can concatenate the list of DataFrames into a single DataFrame using the pd.concat() function. We will also set a two-level index using the ticker symbol and date, and then finally sort based on the indexes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d74bb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stock_data(selected_stock):\n",
    "    st.info(\"Loading Data. This should only take a few seconds.\")\n",
    "\n",
    "    if not os.path.exists(stocks_dir):\n",
    "        # Download stock data if the stocks directory does not exist\n",
    "        download_stock_data()\n",
    "\n",
    "    for filename in os.listdir(stocks_dir):\n",
    "        file_path = join(stocks_dir, filename)\n",
    "\n",
    "        if isfile(file_path) and filename.endswith(\".csv\"):\n",
    "            stock_data = pd.read_csv(file_path)\n",
    "\n",
    "            # Add Ticker column based on the filename (excluding '.csv')\n",
    "            stock_data['Ticker'] = filename[:-4]\n",
    "\n",
    "            # Convert 'Date' column to datetime format\n",
    "            stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "\n",
    "            # Append the data to the list\n",
    "            dataframes.append(stock_data)\n",
    "\n",
    "    # Concatenate individual DataFrames into a single DataFrame\n",
    "    data = pd.concat(dataframes, ignore_index=True)\n",
    "    # Set a MultiIndex using 'Ticker' and 'Date' columns\n",
    "    data.set_index(['Ticker', 'Date'], inplace=True)\n",
    "    # Sort the DataFrame based on the MultiIndex\n",
    "    data.sort_index(inplace=True)\n",
    "\n",
    "    try:\n",
    "        # Use the downloaded S&P 500 data\n",
    "        selected_stock_data = data.loc[selected_stock]\n",
    "        # Call data_cleaning function on the selected stock data\n",
    "        cleaned_data = data_cleaning(selected_stock_data)\n",
    "        return cleaned_data\n",
    "\n",
    "    except Exception as e:\n",
    "        st.error(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeb1882",
   "metadata": {},
   "source": [
    "## Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data_cleaning function in data transformation process focuses on cleaning the data before further use. Rows with missing values or negative values are removed to prevent the use inaccurate data in the predicitive analysis. Duplicate rows are removed for the same reason. The function also removes rows in which the open and close values are greater or less than a logicical value. Finally, the function creates a mask to remove stastical outlier values.\n",
    "\n",
    "\n",
    " We can use an interquartile range (IQR) approach for removing the outliers and prepare it for training. The IQR is a statistical measure that provides a robust indication of the spread of the data. It is defined as the range between the first quartile (25th percentile) and the third quartile (75th percentile) of the data. Because there is some volatility in stock data, we probably may see greater fluxuations in the data then a normally distributed dataset. Therefore, it is probably wise to remain on the side of caution for the threshold value. I settled on a value of 3 since it is relatively strict and almost 99.7% of normally distributed data should fall within 3 standard deviations. This may mean that some outliers will still exist afterwards, but helps ensure we do not remove data excessively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ceefad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(data):\n",
    "    st.info(\"Cleaning Data. This should only take a few seconds.\")\n",
    "\n",
    "    # Make a duplicate of the data\n",
    "    cleaned_data = data\n",
    "    \n",
    "    # Remove rows with missing values\n",
    "    cleaned_data = cleaned_data.dropna()\n",
    "    \n",
    "    # Remove rows with negative values for stock prices and volume\n",
    "    cleaned_data = data[(cleaned_data[['Open', 'High', 'Low', 'Close', 'Adj Close']] > 0).all(axis=1)]\n",
    "    cleaned_data = data[cleaned_data['Volume'] > 0]\n",
    "    \n",
    "    # Remove duplicate rows\n",
    "    cleaned_data = cleaned_data.drop_duplicates()\n",
    "   \n",
    "    # Remove rows where the 'Open' price is greater than the 'High' price\n",
    "    cleaned_data = cleaned_data[cleaned_data['Open'] <= cleaned_data['High']]\n",
    "\n",
    "    # Remove rows where the 'Open' price is less than the 'Low' price\n",
    "    cleaned_data = cleaned_data[cleaned_data['Open'] >= cleaned_data['Low']]\n",
    "\n",
    "    # Remove rows where the 'Close' price is greater than the 'High' price\n",
    "    cleaned_data = cleaned_data[cleaned_data['Close'] <= cleaned_data['High']]\n",
    "\n",
    "    # Remove rows where the 'Close' price is less than the 'Low' price\n",
    "    cleaned_data = cleaned_data[cleaned_data['Close'] >= cleaned_data['Low']]\n",
    "\n",
    "    # Remove rows where 'Low' > 'High'\n",
    "    cleaned_data = cleaned_data[cleaned_data['Low'] <= cleaned_data['High']]\n",
    "\n",
    "    # Remove rows where 'High' < 'Low'\n",
    "    cleaned_data = cleaned_data[cleaned_data['High'] >= cleaned_data['Low']]\n",
    "\n",
    "    # Calculate IQR for each numerical column\n",
    "    iqr_values = cleaned_data.quantile(0.75) - cleaned_data.quantile(0.25)\n",
    "\n",
    "    # Define the lower and upper bounds for outliers\n",
    "    lower_bounds = cleaned_data.quantile(0.25) - 3 * iqr_values\n",
    "    upper_bounds = cleaned_data.quantile(0.75) + 3 * iqr_values\n",
    "\n",
    "    outlier_mask = ~((cleaned_data < lower_bounds) | (cleaned_data > upper_bounds)).any(axis=1)\n",
    "    cleaned_data = cleaned_data[outlier_mask]\n",
    "\n",
    "    return cleaned_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f0a3f0",
   "metadata": {},
   "source": [
    "## Data Loading Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the last step in the ETL process is loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731e586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_raw_data():\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Use the index for the x-axis\n",
    "    last_365_days_data = data.iloc[-365:]\n",
    "\n",
    "    # Plot stock open and close prices\n",
    "    fig.add_trace(go.Scatter(x=last_365_days_data.index.get_level_values('Date'), y=last_365_days_data['Open'], name=\"stock_open\"))\n",
    "    fig.add_trace(go.Scatter(x=last_365_days_data.index.get_level_values('Date'), y=last_365_days_data['Close'], name=\"stock_close\"))\n",
    "\n",
    "    fig.layout.update(title_text='Time Series data with Rangeslider', xaxis_rangeslider_visible=True)\n",
    "    st.plotly_chart(fig)\n",
    "\n",
    "    # Plot RSI\n",
    "    st.subheader('RSI (Relative Strength Index)')\n",
    "    fig_rsi = go.Figure()\n",
    "    fig_rsi.add_trace(go.Scatter(x=last_365_days_data.index.get_level_values('Date'), y=last_365_days_data['RSI'], name=\"RSI\"))\n",
    "    fig_rsi.layout.update(title_text='RSI Indicator', xaxis_rangeslider_visible=True)\n",
    "    st.plotly_chart(fig_rsi)\n",
    "\n",
    "    # Plot Moving Averages\n",
    "    st.subheader('Moving Averages')\n",
    "    fig_ma = go.Figure()\n",
    "    fig_ma.add_trace(go.Scatter(x=last_365_days_data.index.get_level_values('Date'), y=last_365_days_data['SMA50'], name=\"50-day SMA\"))\n",
    "    fig_ma.add_trace(go.Scatter(x=last_365_days_data.index.get_level_values('Date'), y=last_365_days_data['SMA200'], name=\"200-day SMA\"))\n",
    "    fig_ma.layout.update(title_text='Moving Averages', xaxis_rangeslider_visible=True)\n",
    "    st.plotly_chart(fig_ma)\n",
    "\n",
    "    # Plot MACD\n",
    "    st.subheader('MACD (Moving Average Convergence Divergence)')\n",
    "    fig_macd = go.Figure()\n",
    "    fig_macd.add_trace(go.Scatter(x=last_365_days_data.index.get_level_values('Date'), y=last_365_days_data['MACD'], name=\"MACD\"))\n",
    "    fig_macd.add_trace(go.Scatter(x=last_365_days_data.index.get_level_values('Date'), y=last_365_days_data['Signal_Line'], name=\"Signal Line\"))\n",
    "    fig_macd.add_trace(go.Scatter(x=last_365_days_data.index.get_level_values('Date'), y=last_365_days_data['MACD_Histogram'], name=\"MACD Histogram\"))\n",
    "    fig_macd.layout.update(title_text='MACD Indicator', xaxis_rangeslider_visible=True)\n",
    "    st.plotly_chart(fig_macd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a function to change the color of the recommendation box depending on the suggested action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_suggestion(suggestion):\n",
    "    if suggestion == \"Buy\":\n",
    "        return \"green\"\n",
    "    elif suggestion == \"Sell\":\n",
    "        return \"red\"\n",
    "    else:\n",
    "        return \"gray\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For predicting future stock prices, we will utilize Meta's Prophet, a forecasting procedure for series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. \n",
    "\n",
    "\n",
    "We will train the model using the last 365 trading days to predict the next's months's stock prices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daf93e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_stock_prices(data):\n",
    "    # Select the last 365 days of data\n",
    "    last_year_data = data.reset_index().tail(365)[['Date', 'Close']]\n",
    "    last_year_data.columns = ['ds', 'y']\n",
    "\n",
    "    # Initialize the Prophet model\n",
    "    model = Prophet()\n",
    "    model.fit(last_year_data)\n",
    "    future = model.make_future_dataframe(periods=30)  # Predicting for the next 30 days\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    return model, forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49863285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global data  # Declare data as a global variable\n",
    "    st.title('MarketSense Pro')\n",
    "\n",
    "    # Use a text input for the stock ticker symbol\n",
    "    selected_stock = st.text_input('Enter stock ticker symbol for prediction', 'AAPL')\n",
    "\n",
    "    # button to trigger data loading and analysis\n",
    "    if st.button(\"Load Data and Analyze\"):\n",
    "        # Load data\n",
    "        # @st.cache_data\n",
    "\n",
    "        data = load_stock_data(selected_stock)\n",
    "\n",
    "        if data is not None:\n",
    "            st.subheader('Raw data')\n",
    "            st.write(data.tail())\n",
    "\n",
    "            # Calculate RSI\n",
    "            data['RSI'] = ta.momentum.RSIIndicator(data['Close']).rsi()\n",
    "\n",
    "            # Calculate moving averages\n",
    "            data['SMA50'] = ta.trend.SMAIndicator(data['Close'], window=50).sma_indicator()\n",
    "            data['SMA200'] = ta.trend.SMAIndicator(data['Close'], window=200).sma_indicator()\n",
    "\n",
    "            # Calculate MACD using the provided pandas approach\n",
    "            k = data['Close'].ewm(span=12, adjust=False, min_periods=12).mean()\n",
    "            d = data['Close'].ewm(span=26, adjust=False, min_periods=26).mean()\n",
    "            macd = k - d\n",
    "            macd_s = macd.ewm(span=9, adjust=False, min_periods=9).mean()\n",
    "            macd_h = macd - macd_s\n",
    "\n",
    "            # Add MACD data to the DataFrame\n",
    "            data['MACD'] = macd\n",
    "            data['Signal_Line'] = macd_s\n",
    "            data['MACD_Histogram'] = macd_h\n",
    "\n",
    "            # Suggestion of whether to buy or sell based on the indicators\n",
    "            data['Buy_Sell_Signal'] = \"Hold\"  # Default to \"Hold\"\n",
    "            data.loc[(data['RSI'] < rsi_buy_threshold) & (data['MACD'] > data['Signal_Line']), 'Buy_Sell_Signal'] = \"Buy\"\n",
    "            data.loc[(data['RSI'] > rsi_sell_threshold) & (data['MACD'] < data['Signal_Line']), 'Buy_Sell_Signal'] = \"Sell\"\n",
    "\n",
    "            suggestion = data['Buy_Sell_Signal'].iloc[-1]\n",
    "            box_color = color_suggestion(suggestion)\n",
    "\n",
    "            # Display the suggestion\n",
    "            st.markdown(\"<div style='height: 20px;'></div>\", unsafe_allow_html=True)\n",
    "            st.markdown(f'<div style=\"background-color: {box_color}; padding: 10px\"><b>{suggestion}</b></div>', unsafe_allow_html=True)\n",
    "\n",
    "        # Display real-time stock data or a warning if the market is closed\n",
    "        st.markdown(\"<div style='height: 40px;'></div>\", unsafe_allow_html=True)\n",
    "        st.subheader('Real-Time Stock Data')\n",
    "\n",
    "        real_time_stock_data = realtime_stock_data(realtime_api_key, selected_stock)\n",
    "\n",
    "        if real_time_stock_data:\n",
    "            latest_data = real_time_stock_data[list(real_time_stock_data.keys())[0]]\n",
    "            st.write(f\"Real-Time Price ({selected_stock}): ${latest_data['4. close']}\")\n",
    "        else:\n",
    "            st.warning(\"The market is closed and real-time stock data is not available. Try again when the market reopens\")\n",
    "            \n",
    "        # Plot raw data, RSI, moving averages, and MACD\n",
    "        plot_raw_data()\n",
    "\n",
    "        # Predict next year's stock prices using Prophet\n",
    "        result = predict_stock_prices(data)\n",
    "        model = result[0]\n",
    "        forecast = result[1]    \n",
    "\n",
    "        # Display the forecast data and plot\n",
    "        st.subheader('Forecast data')\n",
    "        st.write(forecast.tail())\n",
    "\n",
    "        st.write(f'Forecast plot for 1 year')\n",
    "        fig1 = plot_plotly(model, forecast)\n",
    "        st.plotly_chart(fig1)\n",
    "\n",
    "        # Create section for news\n",
    "        st.subheader('Top News')\n",
    "\n",
    "        # Call the function to get top news\n",
    "        get_top_news(selected_stock)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note these code functions are taken from our main code project file \"main.py\". If you would like to run our stock engine, please refer to that file and follow the instructions at the top. This is because our front-end (GUI) does not run within Jupyter Notebook files.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a08228",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60b12f5",
   "metadata": {},
   "source": [
    "(Conclude by describing any lessons you've learned along the way or suggestions for future students doing similar work. If there is something you wish you knew how to do, this is the place to list it.)\n",
    "\n",
    "\n",
    "While we utilized MACD, we do not have the expertise to tune it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec419449",
   "metadata": {},
   "source": [
    "## Documentation Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dd0fa3",
   "metadata": {},
   "source": [
    "We extensively used the documentation for Yahoo Finance, Alpha Vantage, and News: \n",
    "https://pypi.org/project/yfinance/\n",
    "https://www.alphavantage.co/documentation/\n",
    "https://newsapi.org/docs\n",
    "\n",
    "\n",
    "We used Streamlit's documentation in our frontend development: https://docs.streamlit.io/\n",
    "\n",
    "We also used Investopedia to get the formulas for calulating the MACD, momentum, and RSI indicators: https://www.investopedia.com/terms/\n",
    "\n",
    "Finally, we used Github Copilot to help comment our code functions. "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
